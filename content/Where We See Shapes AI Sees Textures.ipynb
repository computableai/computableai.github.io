{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title: Look at This: Where We See Shapes, AI Sees Textures\n",
    "- summary: CNNs trained in \"the usual way\" tend to learn something different than you might expect. They learn to recognize textures (local structure) rather than shapes (global structure).\n",
    "- author: Daniel Cox\n",
    "- date: 2019-07-16\n",
    "- category: Look at This\n",
    "- image: https://d2r55xnwy6nx47.cloudfront.net/uploads/2019/07/AI_Textures_2880x1220_LHPA.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Series\n",
    "\n",
    "<img src=\"http://weknowmemes.com/wp-content/uploads/2011/12/look-at-this-duck.jpg#right\" style=\"margin-left:15px\" width=\"350\" height=\"268\" />\n",
    "\n",
    "We're starting a simple new series called Look at This, where we briefly plug an article that taught us something.\n",
    "\n",
    "Our first highlight will be a Quanta article about what CNNs learn when trained in \"the usual way\":\n",
    "\n",
    "[Where We See Shapes, AI Sees Textures](https://www.quantamagazine.org/where-we-see-shapes-ai-sees-textures-20190701/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textures, not shapes\n",
    "\n",
    "Training a CNN for object recognition typically involves only showing the algorithm many examples of images that contain or don't contain a target object. Humans also need to see many examples of various objects to get the basic idea. Humans, however, seem to have a bias towards recognition by _shape_ which is missing from CNNs in general.\n",
    "\n",
    "> Geirhos, Bethge and their colleagues created images that included two conflicting cues, with a shape taken from one object and a texture from another: the silhouette of a cat colored in with the cracked gray texture of elephant skin, for instance, or a bear made up of aluminum cans, or the outline of an airplane filled with overlapping clock faces. Presented with hundreds of these images, humans labeled them based on their shape — cat, bear, airplane — almost every time, as expected. Four different classification algorithms, however, leaned the other way, spitting out labels that reflected the textures of the objects: elephant, can, clock.\n",
    "\n",
    "This is a problem worth solving, since the addition of even a small amount of noise can throw off CNN-based classifiers, where humans aren't fooled. \"Adversarial examples\" even do this maliciously, adding exactly the right amount of noise to cause misclassification. So how to fix this?\n",
    "\n",
    "> Geirhos wanted to see what would happen when the team forced their models to ignore texture. The team took images traditionally used to train classification algorithms and “painted” them in different styles, essentially stripping them of useful texture information. When they retrained each of the deep learning models on the new images, the systems began relying on larger, more global patterns and exhibited a shape bias much more like that of humans.\n",
    "\n",
    "![Images painted with alien textures](https://d2r55xnwy6nx47.cloudfront.net/uploads/2019/07/AI_Textures_2880x1220_LHPA.jpg)\n",
    "\n",
    "There were many other insights in this relatively short article, and I commend it to you. It enriched my understanding of what's going on in neural networks, and how far we still need to go to reach parity with humans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
