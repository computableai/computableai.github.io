{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title: arXiv highlights for July 1-6\n",
    "- summary: Beginning a new series highlighting a few interesting RL papers on the arXiv each week. This week: Simple curriculum learning, learning to interact with humans, and warm starting RL.\n",
    "- author: Daniel Cox\n",
    "- date: 2019-07-04\n",
    "- category: arXiv highlights\n",
    "- image: https://www.cornell.edu/assets/core/images/logo-red.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New series\n",
    "\n",
    "This post begins a weekly series highlighting one or more RL papers in the previous week's cs.AI arXiv stream that caught my eye (making no guarantees about the correlation between what catches my eye and what ultimately turns out to be useful, important, etc). I'll be prioritizing sustainability over most other factors, but I do hope to show you some code from time to time. I'm still working out the format, so please leave me feedback in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This week\n",
    "\n",
    "This week's topics:\n",
    "\n",
    "- Growing Action Spaces, by Farquhar et al.\n",
    "- Learning to Interactively Learn and Assist, by Woodward et al.\n",
    "- ProLoNets: Neural-encoding Human Experts' Domain Knowledge to Warm Start Reinforcement Learning, by Silva et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growing Action Spaces\n",
    "\n",
    "This paper (and ProLoNets - see below) piqued my interest because of the sample-efficiency problem in modern DRL. Reinforcement learning algorithms need to interact with the environment quite a bit before they become good at a task, and anything that can shorten this time is of interest. My group is currently working on a learning task with a very low sample rate, so we are actively on the hunt for anything that improves sample efficiency.\n",
    "\n",
    "Growing Action Spaces proposes a form of \"curriculum learning\", where a more complex task is broken down into a sequence of simpler tasks, sometimes by humans, sometimes automatically. In this case, the authors improved the learning speed of their agent by initially giving it fewer actions to work with, training to convergence, and then "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Interactively Learn and Assist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProLoNets: Neural-encoding Human Experts' Domain Knowledge to Warm Start Reinforcement Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
