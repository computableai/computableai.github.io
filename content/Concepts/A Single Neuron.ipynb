{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title: A Single Neuron\n",
    "- summary: Simple math\n",
    "- date: 2019-03-01\n",
    "- status: draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks apply simple math, over and over, to achieve their fantastic results. Their individual pieces are simple, and they combine in a simple way. Let's look at one of those pieces.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A single neuron, also called a \"perceptron\":\n",
    "\n",
    "![Perceptron IO]({static}/images/perceptron1.png)\n",
    "\n",
    "Its mechanics are inspired by biological neurons such as those in your brain, each of which aggregates input from some other neurons and then decides whether or not to \"fire\" its output.\n",
    "\n",
    "An artificial neuron's inputs are numbers (here $x_1$, $x_2$, $x_3$...), and the output is a single number (here $y$). It does very simple math to decide if the incoming inputs are enough for it to activate and fire its own output.\n",
    "\n",
    "## What each neuron does\n",
    "\n",
    "1. Each input has its own \"weight\", which is a number that determines how important that input is to this neuron. Weighing an input is as simple as multiplying the input by its associated weight. In this example, $x_1\\times w_1$, $x_2 \\times w_2$, and $x_3 \\times w_3$ give us the weighted versions of each input:\n",
    "\n",
    "![Perceptron Weights]({static}/images/perceptron2.png)\n",
    "\n",
    "2. Then the neuron adds all of the weighted inputs together. $x_1w_1+x_2w_2+x_3w_3$:\n",
    "\n",
    "![Perceptron Summation]({static}/images/perceptron3.png)\n",
    "\n",
    "3. The last thing it does is wrap the whole sum in some non-linear function, e.g. $f(x_1w_1+x_2w_2+x_3w_3)$. This function can be anything non-linear (that is, the function _isn't_ a line), but the most popular one is also super simple: $f(x)=max(0,x)$. That is, it replaces all negative numbers with zero so the output of the neuron can never be negative. The function $f(x)=max(0,x)$ is called a Rectified Linear Unit (ReLU). You'll see this a lot.\n",
    "\n",
    "![Perceptron Nonlinearity]({static}/images/perceptron4.png)\n",
    "\n",
    "4. And that's it. The output of a single neuron is ultimately $y = f(\\sum\\limits_{i} w_ix_i)$\n",
    "\n",
    "![Perceptron Math]({static}/images/perceptron5.png)\n",
    "\n",
    "An artificial neuron _is_ this equation $y = f(\\sum\\limits_{i} w_ix_i)$. It is not more complicated than that.\n",
    "\n",
    "## How could that _possibly_ be enough?\n",
    "\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
