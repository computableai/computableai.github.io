{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title: arXiv highlights July 28-August 3 2019\n",
    "- summary: Hierarchical RL for concurrent discovery of compound and composable policies.\n",
    "- author: Daniel Cox\n",
    "- date: 2019-08-04\n",
    "- category: arXiv highlights\n",
    "- image: /static/images/arXiv.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This week\n",
    "\n",
    "Just a sketch this week, of [Hierarchical Reinforcement Learning for Concurrent Discovery of Compound and Composable Policies](https://arxiv.org/abs/1905.09668).\n",
    "\n",
    "I've been hearing hierarchical RL mentioned frequently lately, and while I understand it's a way to encode human expertise to achieve otherwise intractible goals, it has also seemed a bit like cheating. However, I have a day job, and this serves as a healthy dose of pragmatism. I also think that even when the goal is fundamental progress, it's often a good idea to achieve the goal _in any way possible_, and then follow-up by working the cheats out of the system one by one. So when I read the abstract of this paper, I was feeling more receptive than previously.\n",
    "\n",
    "Part of what made hierarchical RL seem not worth the cheating was how kludgy and inefficient the usual methods were, retraining a whole new policy from scratch for each subtask. That's why this week's paper caught my eye:\n",
    "\n",
    "> ... we propose an algorithm for learning both compound and composable policies **within the same learning process** by exploiting the off-policy data generated from the compound policy.\n",
    "\n",
    "Their resulting algorithm, \"Hierarchical Intentional-Unintentional Soft Actor-Critic\" (HIU-SAC), efficiently trains all sub-policies simultaneously, choosing actions to perform in the environment using a weighted average of the \"votes\" of all sub-policies, with weights given by a learned selector network (which is _also_ simultaneously trained)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composable hierarchical RL\n",
    "\n",
    "## Architecture\n",
    "\n",
    "<img alt=\"Hierarchical policy diagram\" src=\"{static}/images/policy_network.png#right\" height=\"300px\" width=\"300px\" style=\"margin: 10px\" />\n",
    "\n",
    "The composite policy consists of the individual policy networks, each with its own reward function, trained to take observations $s$ in and output parameters of a conditional Gaussian. There is also a special activation vector selector network trained on the same states to produce weights corresponding to how much each constituent policy applies to the current state. All of these networks share early layers, since they all benefit from an accurate high-level state representation. Finally, some function $f$ takes all of these outputs and determines what action $a$ to _actually_ take in the environment.\n",
    "\n",
    "<img alt=\"Q-value function diagram\" src=\"{static}/images/q_fcn_network.png#left\" height=\"250px\" width=\"250px\" style=\"margin: 10px\" />\n",
    "\n",
    "The Q function networks are similarly arranged, sharing early layers which take a state $s$ and an action $a$ to produce a Q function for each subtask, as well as a composite Q function.\n",
    "<div style=\"clear:both\">&nbsp;</div>\n",
    "\n",
    "## Simultaneous learning\n",
    "\n",
    "> Most methods learn the composable tasks one at a time, and later, the compound task. This procedure is not scalable as all the experience collected for each learning process is only used for that specific process. Also, it is not possible to start learning more complex tasks unless all the compos- able policies have been successfully learned. The method proposed in this section is based on the idea that a single stream of experience can be used to improve not only the policy that is generating the behavior but also, indirectly, many other policies.\n",
    "\n",
    "The authors refer to the composite policy acting as the \"intentional\" policy (the \"behavior\" policy in an off-policy setting), and the composable sub-policies as the \"unintentional\" policies (each one a \"target\" policy in an off-policy setting). They use a variation on SAC to train the composite and composable policies simultaneously within the maximum entropy framework.\n",
    "\n",
    "The objective function for the Q networks simply maximize the expected sum of all mean-squared Bellman errors for each Q network, for each tuple in the replay buffer $\\mathcal{D}$. The objective function for the policy is simply the sum of the objective functions for each intentional and unintentional policy. Each policy objective optimizes the expected difference for each state in $\\mathcal{D}$ between the Q value and log-probability of the selected action (adjustable by temperature $\\alpha$), over all possible actions. HIU-SAC then alternates between policy evaluation and policy improvement steps following SAC.\n",
    "\n",
    "##  The importance of maximizing entropy to adequate exploration\n",
    "\n",
    "It is interesting that the entropy-maximizing RL objective was _absolutely necessary_ for exploring broadly enough to train all of these policies at once.\n",
    "\n",
    "> Note that populating the replay memory buffer with rich experiences is essential for acquiring multiple skills in an off-policy manner. The composable policies learned unintentionally had similar performance than the policies obtained in single-task formulations only when the compound policy was able to efficiently explore the environment. For this reason, the algorithm was built on a maximum entropy RL framework to favor exploration during the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parting thoughts\n",
    "\n",
    "1. In a way, the methods proposed here seem rather obvious, and I found this paper quite easy to understand given that it violated none of my expectations. I also haven't been paying enough attention to hierarchical RL to know off-hand why training the sub-policies in parallel off of the same recorded environment interactions hasn't been tried before (or whether it has been without my notice). Perhaps it was necessary for off-policy RL to reach a level of maturity sufficient for sub-policies to see enough relevant data to train? In any case, don't hear me faulting the authors for trying the obvious. It is relieving a _non_-obvious that a straightforward formulation works so well.\n",
    "2. I'd love to see this work combined with imitation learning and inverse RL to figure out what sub-policies are necessary in the first place from demonstrations. That seems like a very practical framework for real-world learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
