{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title: Individual Neurons\n",
    "- summary: Simple math\n",
    "- date: 2019-03-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks apply simple math, over and over, to achieve their fantastic results. Their individual pieces are simple, and they combine in a simple way. Let's look at one of those pieces.\n",
    "\n",
    "## A single neuron\n",
    "\n",
    "Here's a schematic picture of a single neuron:\n",
    "\n",
    "![Single Neuron IO]({static}/images/singleneuron1.png)\n",
    "\n",
    "Its mechanics are inspired by biological neurons such as those in your brain, each of which aggregates input from some other neurons and then decides whether or not to \"fire\" its output.\n",
    "\n",
    "An artificial neuron's inputs are numbers (here $x_1$, $x_2$, $x_3$...), and the output is a single number (here $y$). It does very simple math to decide if the incoming inputs are enough for it to activate and fire its own output.\n",
    "\n",
    "## What each neuron does\n",
    "\n",
    "1. Each input has its own \"weight\", which is a number that determines how important that input is to this neuron. Weighing an input is as simple as multiplying the input by its associated weight. In this example, $x_1\\times w_1$, $x_2 \\times w_2$, and $x_3 \\times w_3$ give us the weighted versions of each input:\n",
    "\n",
    "![Single Neuron Weights]({static}/images/singleneuron2.png)\n",
    "\n",
    "2. Then the neuron adds all of the weighted inputs together. $x_1w_1+x_2w_2+x_3w_3$:\n",
    "\n",
    "![Single Neuron Summation]({static}/images/singleneuron3.png)\n",
    "\n",
    "3. The last thing it does is wrap the whole sum in some non-linear function, e.g. $f(x_1w_1+x_2w_2+x_3w_3)$. This function can be anything non-linear (that is, the function _isn't_ a line), but the most popular one is also super simple: $f(z)=max(0,z)$. That is, it replaces all negative numbers with zero so the output of the neuron can never be negative. The function $f(z)=max(0,z)$ is called a Rectified Linear Unit (ReLU). You'll see this a lot.\n",
    "\n",
    "![Single Neuron Nonlinearity]({static}/images/singleneuron4.png)\n",
    "\n",
    "4. And that's it. The output of a single neuron is ultimately $y = f(\\sum\\limits_{i} w_ix_i)$\n",
    "\n",
    "![Single Neuron Math]({static}/images/singleneuron5.png)\n",
    "\n",
    "An artificial neuron _is_ this equation $y = f(\\sum\\limits_{i} w_ix_i)$. It is not more complicated than that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll do examples with these soon\n",
    "\n",
    "def relu(z):\n",
    "    return max(z,0)\n",
    "\n",
    "def neuron (w1,w2,w3, x1,x2,x3):\n",
    "    f = relu\n",
    "    return f(x1*w1 + x2*w2 + x3*w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How could that _possibly_ be enough?\n",
    "\n",
    "This may seem quite anticlimactic, and like nothing much has been explained. After all, neural networks can drive cars, play video games, recognize (and generate) human faces, and understand human speech, etc. If each neuron is so simple, and a neural network (as we will see) is just a bunch of neurons arranged in layers, then where does the magic happen?\n",
    "\n",
    "Since this is the post on individual neurons, I'll give the single-neuron answer to that now, and the many-neurons answer to that later. In short, you can shove a lot of computational power into that simple equation, including logic gates and the weighing of evidence.\n",
    "\n",
    "### Neurons can implement universal logic\n",
    "\n",
    "It's just a matter of wiring things up and picking the right weights.\n",
    "\n",
    "You can implement AND and NOT gates with a single neuron apiece. This is significant, because the combination of these two gates is universal for computation. That is, you can build a whole computer if you just have these two gates, and we do.\n",
    "\n",
    "![AND Gate]({static}/images/AND_gate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND(0,0) = 0\n",
      "AND(0,1) = 0\n",
      "AND(1,0) = 0\n",
      "AND(1,1) = 1\n"
     ]
    }
   ],
   "source": [
    "AND = lambda IN1,IN2: neuron(w1=-1,w2=1,w3=1, x1=1,x2=IN1,x3=IN2)\n",
    "\n",
    "print(\"AND(0,0) =\", AND(0,0))\n",
    "print(\"AND(0,1) =\", AND(0,1))\n",
    "print(\"AND(1,0) =\", AND(1,0))\n",
    "print(\"AND(1,1) =\", AND(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NOT Gate]({static}/images/NOT_gate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT(0) = 1\n",
      "NOT(1) = 0\n"
     ]
    }
   ],
   "source": [
    "NOT = lambda IN1: neuron(w1=1,w2=0,w3=-1, x1=1,x2=0,x3=IN1)\n",
    "\n",
    "print(\"NOT(0) =\", NOT(0))\n",
    "print(\"NOT(1) =\", NOT(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons can represent the weighing of evidence\n",
    "\n",
    "Neurons are also capable of expressing _more_ than binary logic. They can represent gradations of certainty and real values. Here a neuron weighs the evidence that the incoming numbers represent a cat, and outputs not just a binary determination, but a degree of \"catness\".\n",
    "\n",
    "Again, it's just a matter of wiring things up and picking the right weights.\n",
    "\n",
    "![Catness Components]({static}/images/catness_components.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catness = 50.0\n"
     ]
    }
   ],
   "source": [
    "catness = lambda pt, fm, bdn: neuron(w1=0.5,w2=0.2,w3=-0.5, x1=pt,x2=fm,x3=bdn)\n",
    "\n",
    "# Say some image has these values...\n",
    "pointy_topness = 70\n",
    "fuzzy_middleness = 80\n",
    "big_dark_noseness = 2\n",
    "\n",
    "result = catness(pointy_topness, fuzzy_middleness, big_dark_noseness)\n",
    "\n",
    "print(\"catness =\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Wiring things up and picking the right weights\"\n",
    "\n",
    "I hope I've convinced you that if you have the right wiring and weights, simple neurons can perform complex feats of wonder in combination. Wiring things up right is hard, but humans generally do it manually. Picking the right weights, however, is highly non-trivial to do manually, and humans generally _don't_ try to do it themselves. More on this later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
