{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title: Active Perception in Adversarial Scenarios\n",
    "- summary: Accumulating evidence about peers to discriminate potential threats.\n",
    "- author: Daniel Cox\n",
    "- date: 2019-09-22\n",
    "- category: arXiv highlights\n",
    "- image: /static/images/arXiv.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This week\n",
    "\n",
    "This week's paper is [Active Perception in Adversarial Scenarios using Maximum Entropy Deep Reinforcement Learning](https://arxiv.org/abs/1902.05644v1). The idea is that an agent interacting with another agent can learn to assess the threat it may pose. It does this by actively testing the opponent agent's behavior, and does not assume the opponent's behavior remains stationary. It uses Bayesian filtering to update its belief about the disposition of the opponent, and that's why this paper caught my eye. I'm on a Bayesian kick lately.\n",
    "\n",
    "> To summarize, the contribution here is the development of a scalable robust active perception method in scenarios where a potential adversary opponent could be actively hostile to the intent recognition activity, which extends and outperforms the POMDP methods.\n",
    "\n",
    "I'm a bit short on time this week, so I apologize for the amount of jargon and the unusually high level of confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem setup\n",
    "\n",
    "> We model the active perception problem as a planning problem, defined by the tuple $\\langle S,A^a,A^o,T,O,R,b_0,\\gamma \\rangle$, where $S=\\langle S^o,S^p \\rangle$ is the state of the world, consisting of the set of observable states $S^o$ and the set of partially observable states $S^p$; $A^a$ is the set of actions of the autonomous agent; $A^o$ is the set of actions of the opponent; we further assume that regardless of the intention, the opponent has the same set of observable actions. Otherwise, an intention is easily identifiable once an action that is uniquely corresponding to that type of intention is observed. $T:S \\times A^a \\times A^o \\rightarrow \\Delta_S $ is the transition probability, where $\\Delta_{\\bullet}$ denotes the space of probability distribution over the space $\\bullet$. $O: S \\times A^a \\rightarrow \\Delta_{A^o}$ is the observation probability; $R: S \\times A^a \\times A^o \\rightarrow  \\mathbb{R}$ is the reward function; $b_0$ is the prior probability of the opponent being an adversary; and $\\gamma$ is the discount factor.\n",
    "\n",
    "Further, the opponent is assumed to be either neutral (merely self-interested, in a known way) or hostile (goal-directed, as defined by a known MDP), with bounded rationality, (it may not be able to take the optimal action) and it is likely to behave deceptively.\n",
    "\n",
    "Notice that the actual behavior of the opponent is known if its disposition is known, which to my mind may or may not be a reasonable assumption, depending on the setting. Since I've had AI safety on the brain lately, it strikes me as _unrealistic_ in a situation where your opponent is smarter than you are. It may be more realistic in settings where everyone has the same goal and it's relatively clear how anyway would try to achieve it if they didn't have to deal with other agents.\n",
    "\n",
    "The authors' adversarial model is interesting. ($\\lambda$ is the parameter to $\\pi^o$ that specifies whether the agent is neutral: $\\lambda=0$, or adversarial: $\\lambda=1$):\n",
    "\n",
    "> We use the following equation to model an adversarial agent's policy $\\pi^o$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pi^o(a^o_t|s_t,\\lambda=1;\\alpha,\\beta)= & \\text{argmin}_{\\pi \\in \\Delta} \\{\\mathbb{KL}(\\pi|\\pi^{\\text{MDP}}_{\\alpha})\\\\\n",
    "& +\\beta \\mathbb{KL}(\\pi|\\pi^o(\\cdot|s_t,\\lambda=0)) \\}\n",
    "\\pi^{\\text{MDP}}_{\\alpha}(a_t^o|s_t,\\lambda=1)=e^{\\alpha Q(s_t,a_t^o)}/Z(s_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The thing to take away from this is that both rationality and deception are tunable parameters. The rationality of the opponent is controlled by the temperature parameter $\\alpha$, by adjusting how well the opponent makes use of the optimal Q function. The degree to which the opponent is _deceptive_ is controlled by $\\beta$, which adjusts how much the KL-divergence of the existing policy from the _neutral_ policy affects the opponent's search for an optimal strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian filtering\n",
    "\n",
    "> We maintain a belief $b_t(\\lambda)$ over the hidden variable by Bayesian filtering.\n",
    "\n",
    "As I mentioned, I'm rather short on time today, so I must apologize again for not actually spending the time to explain this. For now, suffice it to say that the opponent is either neutral ($\\lambda=0$) or hostile ($\\lambda=1$), and how your agent reacts to it depends very much on which one of those it believes it is playing against. Bayesian filtering will allow it to make the most of the evidence available, so it can use its best guess as it trains. \n",
    "\n",
    "> We define a hybrid belief-state dependent reward to balance exploration and safety\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "r(b_t,s_t,a^a_t)&=-H(b_t)+r(s_t,a^a_t)\\\\\n",
    "&=b\\log b+(1-b)\\log(1-b)+r(s_t,a^a_t),\n",
    "\\end{aligned}\n",
    "\\label{eq6}\n",
    "\\end{equation}\n",
    "where we use the shorthand $b$ to denote $b_t(\\lambda=1)$, the belief that the opponent is an adversary; and $r(s_t,a^a_t)$ is the state dependent reward.\n",
    "\n",
    ">This reward balances exploration behavior and safety. The negative entropy reward $-H(b_t)$ can be interpreted as maximizing the expected logarithm of true positive rate (TPR) and true negative rate (TNR). The state-dependent reward $r(s_t,a^a_t)$ depends both on the observable state and the partially observable intent state $\\lambda$, as well as the action of the autonomous agent. This reward \n",
    "is used to ensure safety. For instance, some actions could be dangerous to the neutral [opponent], which are discouraged by a large negative reward.\n",
    "\n",
    "Our agent is trained using [Soft-Q Learning](https://arxiv.org/abs/1702.08165) while values of $\\lambda$ are varied, with corresponding opponent behavior. Interestingly, in the case study section the authors mention that the actual adversary models were not always provided in the learning phase.\n",
    "\n",
    "> The active perception agent has to identify the hidden intent while bein grobust to this model uncertainty, which is challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parting thoughts\n",
    "\n",
    "1. I admit to being a bit confused by this paper. The authors claim to do Bayesian filtering, but it's not an explicit feature of the algorithm. In fact, they seem to be sampling $\\lambda$ for use in training by using only $b_0$, their prior probability for their belief state. Perhaps it's a typo.\n",
    "2. They also seem to claim that the two models of the opponent behavior must be known, but then they mention they're not available during the learning phase in their case study. Drop me a line if this makes sense to you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
